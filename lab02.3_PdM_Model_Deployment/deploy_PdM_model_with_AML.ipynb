{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model deployment to ACI\n",
    "\n",
    "We finished the last Notebook by finding best fitting model using automated ML and registering it to our Azure ML account. In this Notebook, we deploy this model to an ACI instance and test it by scoring data against it. Scoring here happens in near-realtime, meaning that the data we score is pre-computed for us (such as a nightly batch job). Scoring can also happen in realtime, but as we will explore in a later Notebook, this requires more work. For predictive maintenance, realtime scoring is usually not needed, because models are used to predict when a machine is going to *about to* fail, which gives us some time to run unscheduled maintenance and replace parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup we have already created an AML workspace. Let's load the workspace and create an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the workspace directly from the config file we created in the early part of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../lab02.0_PdM_Setting_Up_Env/aml_config/config.json'\n",
    "ws = Workspace.from_config(config_path)\n",
    "\n",
    "experiment_name =  'pred-maint-automl' # choose a name for experiment\n",
    "project_folder = '.' # project folder\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(data=output, index=['']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 0.1.74\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the test data, not to evaluate the model but only to use it to get some predictions from our deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X_test\n",
    "%store -r y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scoring script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the deployment consists of pointing to the model we want to deploy. We can simply provide the model name, which was given to us at the time we registered the model. We can also go to the Azure portal to look up the model name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick sanity check to ensure that the model exists and can be loaded (loading the model in the current session is not required for deployment).\n",
    "\n",
    "**Note**: You have to updated the `model_name` below. You can find the model name in your workspace in the azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model_name = \"AutoML4b8dd2f49best\"\n",
    "\n",
    "model = Model(workspace=ws, name=model_name)\n",
    "print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a scoring script that will run every time we make a call to the deployed model. The scoring script consists of an `init` function that will load the model and a `run` function that will load the data we provide at score time and use the model to obtain predictions.\n",
    "\n",
    "**Note**: You have to updated the model_name below. You can find the model name in your workspace in the azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "import azureml.train.automl\n",
    "\n",
    "model_name = \"AutoML4b8dd2f49best\"\"\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path(model_name = model_name) # this name is modeld.id of model that we want to deploy\n",
    "    # model_path = Model.get_model_path('model.pkl') # select this if deploying model from file\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(rawdata):\n",
    "    try:\n",
    "        data = json.loads(rawdata)['data']\n",
    "        data = numpy.array(data)\n",
    "        result = model.predict(data)\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return json.dumps({\"error\": result})\n",
    "    return json.dumps({\"result\":result.tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a conda environment file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by retrieving the run ID for the automl experiment we ran in the last Notebook and pasting it in for the `run_id` argument in the `AutoMLRun` function below.\n",
    "\n",
    "**Note**: You have to updated the run_id below. You can find the run_id under the experiments in the azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_run = AutoMLRun(experiment=experiment, run_id='AutoML_4b8dd2f4-9a88-4a90-b427-9c45aee54287')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a `yml` file for the conda environment that will be used to run the scoring script above. To ensure consistency of the scored results with the training results, the SDK dependencies need to mirror development environment (used for model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = ml_run.get_run_sdk_dependencies(iteration=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ['azureml-train-automl', 'azureml-sdk', 'azureml-core']:\n",
    "    print('{}\\t{}'.format(p, dependencies[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['scikit-learn'])\n",
    "# myenv.add_pip_package(\"azureml-train\")\n",
    "# myenv.add_pip_package(\"azureml-train-automl\")\n",
    "# myenv.add_pip_package(\"azureml-core\")\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat myenv.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile myenv.yml\n",
    "name: myenv\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - pip:\n",
    "    - scikit-learn==0.19.1\n",
    "    - azureml-sdk[automl]==0.1.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile myenv.yml\n",
    "name: myenv\n",
    "dependencies:\n",
    "  - scikit-learn==0.19.1\n",
    "  - pip:\n",
    "    - azureml-defaults\n",
    "    - azureml-train-automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat myenv.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scoring script and conda environment file, we can now create a docker image that will host the scoring script and a Python executable that meets the conda requirement dependencies laid out in the YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(runtime= \"python\",\n",
    "                                 execution_script=\"score.py\",\n",
    "                                 conda_file=\"myenv.yml\",\n",
    "                                 tags = {'area': \"digits\", 'type': \"automl_classification\"},\n",
    "                                 description = \"Image for automl classification sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image_name = experiment_name + \"-img\"\n",
    "\n",
    "image = Image.create(name = image_name,\n",
    "                     models = [model], \n",
    "                     image_config = image_config, \n",
    "                     workspace = ws)\n",
    "\n",
    "image.wait_for_creation(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the image creation fails, this is how we can access the log file and examine what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.image_build_log_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the image location that will be used when the imaged is pulled down from Docker hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.image_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the image was created in another session and we just wanted to point to it in this session, then we can just pass the image name and workspace to the `Image` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image(name = experiment_name + \"-img\", workspace = ws, version=29)\n",
    "print(image.image_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Image as web service on ACI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to deploy our image as a web service on ACI. To do so, we first create a config file and then pass it to `deploy_from_image` along with a name for the service, the image we created in the last step, and our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"method\" : \"automl\"}, \n",
    "                                               description='Predictive maintenance using auto-ml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a service with the same name already exists, we can delete it by calling the `delete` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = experiment_name + \"-aci\"\n",
    "print(aci_service_name)\n",
    "# aci_service.delete()\n",
    "aci_service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
    "                                           image = image,\n",
    "                                           name = aci_service_name,\n",
    "                                           workspace = ws)\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can get logs from the deployed service, which can help us debug any issues that cause the deployment to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = aci_service.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "ll = re.findall(r\"\\{.*\\}\", logs)\n",
    "dd = [json.loads(l) for l in ll]\n",
    "\n",
    "for k in dd:\n",
    "    if 'level' in k.keys():\n",
    "        if k['level'] == 'ERROR': \n",
    "            print('================================================================================')\n",
    "            # pprint(json.loads(k['message']))\n",
    "            print(json.loads(k['message'])['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative deployments (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other ways that we could have launched our ACI deployment. The first one is by deploying directly from the image config file and the registered model. In this scenario, the deployment will first create the image from the registered model, and then deploy the docker container from the base image. So we combine two steps (image creation, service creation) into a single step. However, behind the scenes the steps still run individually and create corresponding resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aci_service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = experiment_name + \"-aci\"\n",
    "print(aci_service_name)\n",
    "\n",
    "# aci_service = Webservice.deploy_from_model(deployment_config = aciconfig,\n",
    "#                                        image_config = image_config,\n",
    "#                                        models = [model], # this is the registered model object\n",
    "#                                        name = aci_service_name,\n",
    "#                                        workspace = ws)\n",
    "# aci_service.wait_for_deployment(show_output = True)\n",
    "# print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we launched the ACI docker container from the image config file and the registered model. But we can take one further step back and simply provide the model pickle file and let it register the model, then create a docker image from it and finally launch an ACI docker container from the image. In this case, we are combining three steps into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aci_service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = experiment_name + \"-aci\"\n",
    "print(aci_service_name)\n",
    "\n",
    "# aci_service = Webservice.deploy(deployment_config = aciconfig,\n",
    "#                                 image_config = image_config,\n",
    "#                                 model_paths = ['model.pkl'],\n",
    "#                                 name = aci_service_name,\n",
    "#                                 workspace = ws)\n",
    "\n",
    "# aci_service.wait_for_deployment(show_output = True)\n",
    "# print(aci_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining many steps into one may save us a few lines of code, but it has the disadvantage of appearing to over-simplify the workflow. So it is probably best to avoid doing it, especially for production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Web Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to test our web service. To begin with, we will point to our service using `Webservice`. Note that we've already done this in the last step, so in the current session this is not a necessary step, but since we want to be able to test the service from any Python session, we will point to the service again here. There is next to no overhead in doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "aci_service_name = experiment_name + \"-aci\"\n",
    "# image = Image(name = experiment_name + \"-img\", workspace = ws)\n",
    "\n",
    "aci_service = Webservice(workspace = ws, name = aci_service_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to testing the service. To do so, we will take a few random samples from `X_test` and dump its content into a json string (with UTF-8 encoding). This will act as the data that we intend to score. We can pass this data to the service using the `run` method, and it will return the predictions to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "sample_indices = np.random.permutation(X_test.shape[0])[0:n]\n",
    "\n",
    "test_samples = json.dumps({\"data\": X_test.iloc[sample_indices, :].values.tolist()})\n",
    "test_samples = bytes(test_samples, encoding = 'utf8')\n",
    "print(test_samples)\n",
    "\n",
    "# predict using the deployed model\n",
    "prediction = aci_service.run(input_data = test_samples)\n",
    "print('**********************************************')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we took the data to be scored directly from `X_test`. But this data had already been pre-processed for us and was ready for scoring. A more realistic scenario involves getting raw data, pre-processing it and then feeding it to the deployed model for scoring. In this lab, we will implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the telemetry data\n",
    "# run anomaly detection on it (invoke score_AD.py to get errors)\n",
    "# compute moving average telemetries (use the function in score_AD.py for computing MAs at test time)\n",
    "# append maintenance history and failure history to it\n",
    "# now you're ready to score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
